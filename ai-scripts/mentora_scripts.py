# -*- coding: utf-8 -*-
"""Mentora_Scripts.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F-BI-bgOMaqXPx-6FDM09hbY-WaIitzU
"""

import json
import csv
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import numpy as np

print(" loading file...")
with open('leaf.jsonl', 'r', encoding='utf-8') as f:
    lines = f.readlines()

data = []
for line in lines:
    obj = json.loads(line)
    data.append({
        'split': obj['split'],
        'essay_text': obj['essay_text'],
        'human_feedback': obj['human_feedback_text']
    })

print(f"loaded {len(data)} essays.")

print("loading model...")
model_name = "KevSun/Engessay_grading_ML"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.eval()
print(f"ðŸš€ Model loaded on: {device}")

def get_grades(text):
    if not text or not isinstance(text, str) or text.strip() == "":
        return [1.0] * 6  # Default fail-safe values
    encoded_input = tokenizer(
        text, return_tensors='pt', padding=True, truncation=True, max_length=64
    ).to(device)

    with torch.no_grad():
        outputs = model(**encoded_input)

    predictions = outputs.logits.squeeze().cpu().numpy()
    item_names = ["cohesion", "syntax", "vocabulary", "phraseology", "grammar", "conventions"]
    scaled_scores = 2.25 * predictions - 1.25
    rounded_scores = [round(score * 2) / 2 for score in scaled_scores]
    return rounded_scores

print("starting grading...")
for i, item in enumerate(data):
    essay = item['essay_text']
    scores = get_grades(essay)
    item['grade'] = scores
    if i % 50 == 0:
        print(f"processed {i}/{len(data)} essays...")


def write_csv(filtered_data, filename):
    with open(filename, 'w', newline='', encoding='utf-8') as f:
        fieldnames = ['essay_text', 'human_feedback', 'grade']
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()

        for row in filtered_data:
            # Only keep expected fields
            cleaned_row = {key: row[key] for key in fieldnames}
            writer.writerow(cleaned_row)


train_data = [d for d in data if d['split'] == 'train']
test_data  = [d for d in data if d['split'] == 'test']

print("writing csv files...")
write_csv(train_data, 'training.csv')
write_csv(test_data, 'testing.csv')

print("done! files saved: training.csv and testing.csv")

!pip install transformers datasets torch pandas scikit-learn --quiet

from datasets import Dataset
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer

train_df = pd.read_csv("training.csv").dropna(subset=["essay_text", "human_feedback"]).head(150)
test_df = pd.read_csv("testing.csv").dropna(subset=["essay_text", "human_feedback"]).head(150)

train_df["grading_pattern"] = train_df["human_feedback"].apply(
    lambda f: "Extract and list specific rubric-based patterns the teacher follows. For example:\n- Strict on thesis clarity\n- Tolerant of minor grammar errors\n- Values personal voice\nFeedback: " + f
)

test_df["grading_pattern"] = test_df["human_feedback"].apply(
    lambda f: "Extract and list specific rubric-based patterns the teacher follows. For example:\n- Strict on thesis clarity\n- Tolerant of minor grammar errors\n- Values personal voice\nFeedback: " + f
)

train_dataset = Dataset.from_pandas(train_df[["essay_text", "grading_pattern"]])
test_dataset = Dataset.from_pandas(test_df[["essay_text", "grading_pattern"]])

model_checkpoint = "google/flan-t5-small"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

max_input_length = 512
max_target_length = 256

def preprocess(example):
    inputs = tokenizer(example["essay_text"], truncation=True, padding="max_length", max_length=max_input_length)
    targets = tokenizer(example["grading_pattern"], truncation=True, padding="max_length", max_length=max_target_length)
    inputs["labels"] = targets["input_ids"]
    return inputs

train_tokenized = train_dataset.map(preprocess, remove_columns=train_dataset.column_names)
test_tokenized = test_dataset.map(preprocess, remove_columns=test_dataset.column_names)

model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)

from huggingface_hub import login
login()

training_args = TrainingArguments(
    output_dir = "./flan-t5-feedback",
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=3,
    save_steps=500,
    logging_steps=100,
    eval_strategy="epoch",
    save_strategy="epoch",
    weight_decay=0.01,
    load_best_model_at_end=True,
    push_to_hub=True,
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_tokenized,
    eval_dataset=test_tokenized,
    tokenizer=tokenizer
)

trainer.train()

def generate_insights(essay):
    inputs = tokenizer(essay, return_tensors="pt", truncation=True, padding=True, max_length=max_input_length)
    outputs = model.generate(**inputs, max_new_tokens=256)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)


sample_essay = test_df.iloc[0]["essay_text"]
print(generate_insights(sample_essay))
print(sample_essay)

# Save model and tokenizer to a folder inside your Colab environment
model.save_pretrained("./my-fine-tuned-model")
tokenizer.save_pretrained("./my-fine-tuned-model")

import shutil

# Zip the folder
shutil.make_archive("my-fine-tuned-model", 'zip', "./my-fine-tuned-model")

from google.colab import files

# Download the zipped model folder
files.download("my-fine-tuned-model.zip")

!pip uninstall -y torch torchvision torchaudio
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

from bertopic import BERTopic
import pandas as pd

# 1. Input: Essay feedback samples
feedback_sentences = [
    "Dear student, your discussion would have been better if you had tried to follow the proper discussion outline as indicated in the original prompt.",
    "You neglected to properly assess the requirements of the essay and represent it in your outline plus paragraph discussion.",
    "You were asked to discuss 2 points of view and then your personal opinion. You only discussed one point of view and your opinion.",
    "In terms of task accuracy, that would result in a score of 4 because your response to the required discussion is minimal.",
    "The failure of your opening statement, the paraphrasing sealed the failing score for your TA portion."
]

teacher_ids = ["teacher_A"] * len(feedback_sentences)

# 2. Train BERTopic model
topic_model = BERTopic(umap_model=None)
topics, probs = topic_model.fit_transform(feedback_sentences)

# 3. Analyze topic labels
topic_info = topic_model.get_topic_info()
print("\n=== Topic Labels ===")
print(topic_info)

# 4. Classify feedback as strict or not (you can replace this with ML)
sentiments = [
    "strict" if any(word in sent.lower() for word in ["better", "neglected", "failing"]) else "neutral"
    for sent in feedback_sentences
]

# 5. Create full DataFrame
df = pd.DataFrame({
    "teacher": teacher_ids,
    "sentence": feedback_sentences,
    "topic_id": topics,
    "sentiment": sentiments
})

# 6. Map topic ID to BERTopic label
topic_label_map = {row["Topic"]: row["Name"] for _, row in topic_info.iterrows()}
df["topic_label"] = df["topic_id"].map(topic_label_map)

# 7. Aggregate strictness per topic
agg = df.groupby(["teacher", "topic_label", "sentiment"]).size().unstack(fill_value=0)
agg["total"] = agg.sum(axis=1)
agg["strict_ratio"] = agg.get("strict", 0) / agg["total"]

# 8. Display profile
profile = agg.reset_index()[["teacher", "topic_label", "strict_ratio"]]
print("\n=== Grading Style Profile ===")
print(profile)

from collections import Counter
import re

def get_top_keywords(sentences, n=5):
    words = []
    for sent in sentences:
        words += re.findall(r'\b\w+\b', sent.lower())
    most_common = Counter(words).most_common(n)
    return [word for word, _ in most_common if word not in {"the", "you", "and", "your"}]

for cluster_id in sorted(set(clusters)):
    sents = df[df["cluster"] == cluster_id]["sentence"].tolist()
    keywords = get_top_keywords(sents)
    print(f"Topic {cluster_id}: {' / '.join(keywords)}")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.utils import to_categorical

df = pd.read_csv("grammar4.csv")
df = df.drop_duplicates(subset=['text'])

for i, row in df.iterrows():
  if row["label"] == "Organization & Structure":
    df.drop(i, inplace=True)

df.reset_index(drop=True, inplace=True)

print(df.head)

X = df['text']
y = df['label']

vectorizer = TfidfVectorizer()
X_vectorized = vectorizer.fit_transform(X)

le = LabelEncoder()
y_encoded = le.fit_transform(y)
y_categorical = to_categorical(y_encoded, num_classes=3)

X_train, X_test, y_train, y_test = train_test_split(
    X_vectorized, y_categorical, test_size=0.2, random_state=42
)
X_train_dense = X_train.toarray()
X_test_dense = X_test.toarray()

model = Sequential([
    Dense(128, activation="relu", input_shape=(X_train_dense.shape[1],)),
    Dropout(0.3),
    Dense(64, activation="relu"),
    Dropout(0.3),
    Dense(3, activation="softmax")
])

model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

early_stop = EarlyStopping(monitor='val_loss', patience=3)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)

history = model.fit(X_train_dense, y_train,
                    validation_split=0.2,
                    epochs=15,
                    batch_size=32,
                    callbacks=[early_stop, reduce_lr])

y_pred_probs = model.predict(X_test_dense)
y_pred = np.argmax(y_pred_probs, axis=1)
y_true = np.argmax(y_test, axis=1)

acc = accuracy_score(y_true, y_pred)
cm = confusion_matrix(y_true, y_pred)

print(f"Accuracy: {acc}")

import matplotlib.pyplot as plt
import seaborn as sns

class_names = le.classes_

plt.figure(figsize=(7,6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=class_names, yticklabels=class_names)

plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss', color='blue')
plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')
plt.title('Loss Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

# Plot Accuracy
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Training Accuracy', color='blue')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='orange')
plt.title('Accuracy Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

sentence = ["Make sure every paragraph ties back to your argument â€” some ideas feel disconnected."]
vec = vectorizer.transform(sentence).toarray()
prediction = model.predict(vec)
predicted_class = le.inverse_transform([np.argmax(prediction)])
print("Predicted Category:", predicted_class[0])

def predict(sentence):
    vec = vectorizer.transform([sentence]).toarray()  # <-- wrap sentence in a list
    prediction = model.predict(vec)
    predicted_class = le.inverse_transform([np.argmax(prediction)])
    return predicted_class[0]  # return instead of print

import spacy
nlp = spacy.load("en_core_web_sm")

def sent_tokenize(text):
    doc = nlp(text)
    return [sent.text.strip() for sent in doc.sents]

def loop_through(feedback):

    sentences = sent_tokenize(feedback)

    for i, sentence in enumerate(sentences, 1):
        print(f"Sentence {i}: {sentence}")
        print(predict(sentence))

f = "Great connections! You explain them very well."
loop_through(f)

from transformers import pipeline

sentiment_analyzer = pipeline("sentiment-analysis", model="distilbert-base-uncased-finetuned-sst-2-english")

result = sentiment_analyzer("There are many grammar errors that must be fixed.")
print(result)

!pip install -q spacy
!python -m spacy download en_core_web_sm